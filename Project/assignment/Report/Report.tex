\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{cite}
\usepackage{tabularx}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

\title{Using neural network to detect native versus non-native English Speakers}

\name{Kanan Schmid, Andrew Braun, Adam Belhouchat}
\address{UCLA ECE 114 -- Speech and Image Processing. Professor F. Lorenzelli}

\begin{document}
%\ninept

\maketitle

\begin{abstract}
	In this project, we trained a neural network to differentiate between native and non-native English speakers given speech signals of the same sentence.
	We extracted MFCCs from each signal and fed them into a feed-forward neural network and an RNN.
	Throughout the report, the analysis and accuracy of many different methods to compare speech will be discussed. 
	Our best results were achieved using a feed-forward neural network given mean and standard deviation of MFCCs and an RNN given a time sequence of MFCCs.
\end{abstract}

\begin{keywords}
feed-forward neural network,
RNN,
LSTM,
MFCC,
LPC
\end{keywords}

\section{Introduction}
\label{sec:intro}

Neural networks have found widespread use in speech recognition.
Convolutional neural networks \cite{microsoft} and recurrent neural networks (RNNs), specifically long short-term memory units (LSTM) \cite{google}, are two types of networks that have found some success. 
In this project, we used a data set of 170 speech signals, each of which is a different individual speaking the same sentence, to train a feed-forward neural network and an RNN to classify native versus non-native English speakers and compare the performance of the two neural networks. 

Our approach was to figure out what the defining characteristics of different accents are and extract these characteristics from the speech signal.
We researched different features and characteristics that could effectively separate native and non-native English speakers.
These features would then be extracted from the speech signal and fed into the neural net, which would classify the features as coming from a native or a non-native English speaker.

\section{Unsuccessful trials}
\label{sec:unsuccessful}

In this section, we will briefly describe the many different features and analyses that were ineffective or less effective than the final features.

\subsection{Speech normalization}
\label{subsec:normalization}

One of the biggest hurdles in this project was ensuring that the same parts of the signals were being compared.
For example, if we compare two different signals at 11 seconds in, it is unlikely they will be saying the same thing at that time.
To get around this, there must be a way to  either choose features that encompass the whole signal (like pitch, which should be relatively constant throughout), or find some way to normalize the signals.
The most common way of doing this is dynamic time warping.
Dynamic time warping compares two similar signals of different duration and attempts to normalize them in the time domain.

Our issue was that performing dynamic time warping on these long signals, some with over a million samples, was computationally impossible.
MATLAB would need to perform massive matrix multiplications that would take up multiple terabytes in memory.
To get around this, we tried time warping short frames of the signal, each only a few milliseconds at most, but this would distort the frames to the point of being unrecognizable.
Moreover, since we had more than two signals, it would be difficult to perform dynamic time warping that would make them all equivalent.
We attempted to use Euclidean distance from the training data to three known English speakers that had non-noisy speech, different accents, and different voices (one male American, one female British, one male Irish), but unfortunately, this feature did not correlate well with actual accent and during training gave a testing error of ~52\%.

\subsection{Vowel extraction}
\label{subsec:vowel}

We attempted to use DARLA’s vowel extraction tools \cite{DARLA} to extract vowel formants from the speech files so we would be able to easily compare vowels across different accents.
However, their vowel extraction tool was not consistent enough on our dataset.
Due to the wide range in voice types and recording quality, it could not extract all vowels for each signal, even after denoising the speech files, and for some files it would completely fail to process.
Therefore we could not obtain a set of all vowels and their formants for all speech files, so we could not make use of this tool in our final network.

\subsection{Time-domain / FFT hybrid network}
\label{subsec:fft}

We also explored the use of a hybrid network, given in \Cref{fig:hybrid}.
Using raw FFT / time-domain features gave low accuracies (50\%, essentially guessing) and was computationally expensive.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 8.5cm]{figs/hybrid_network}
	\caption{Diagram of hybrid network.
	Various activation functions were explored in this architecture, however testing validation results were always low.}
	\label{fig:hybrid}
\end{figure}

\subsection{Filter bank energies and LPCs}
\label{subsec:fbe}

Lastly, we attempted to use filter bank energies and linear predictive coefficients (LPCs) in our neural network.
Previous research has shown success using filter bank energies (FBEs) for accent classification and speech recognition \cite{Paliwal, chuaccent}, and LPCs have been used for speaker recognition \cite{lpc}.
However, when we tested them in our implementation we did not see any improvement in testing accuracy and occasionally even saw a decrease.
We attempted to use FBEs both by themselves and in addition to mel-frequency cepstral coefficients (MFCCs), as well as LPCs by themselves or with MFCCs and FBEs, but none of these attempts gave promising results.

\section{Most successful results}
\label{sec:success}

\subsection{Feed-forward neural network: MFCC average and standard deviation}
\label{subsec:feedforward}

Our decision to use MFCCs as a feature was based off previous speech research done in the field: MFCCs have been previously used for Gaussian Mixture Models and other speech research \cite{mfccs, chuaccent}.
We hypothesized that non-native English speakers would be more likely to have a higher variation in MFCCs across each frame, as we expected it would be more likely for non-native English speakers to make more “mistakes” in the way they speak.
Our implementation was a feed-forward network consisting of four dense layers with sigmoid activations.
The mean and standard deviation of the MFCCs over all the frames were fed into the network and achieved testing accuracy (depending on the train / test split) anywhere from 58 to 69\%.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 2.15cm]{figs/ff_architecture}
	\caption{A diagram of the feed-forward neural network architecture.}
	\label{fig:ff_architecture}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width = 8.5cm]{figs/ff_graph}
	\caption{A graph of averaged training error versus testing error over five different splits for the feed-forward neural network implementation.}
	\label{fig:ff_graph}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width = 8.5cm]{figs/best_ff_acc}
	\caption{A graph of our best training error versus testing error for the feed-forward neural network implementation.}
	\label{fig:best_ff}
\end{figure}

The performance of this network was comparable to the RNN implementation.
However, this implementation was more susceptible to errors in labeling and quantity of positive and negative examples of native English speakers.
Note that overfitting occurs past epoch 40, however implementation of an  l1 / l2 regularizer did not sufficiently control this overfitting.

\subsection{LSTM RNN implementation with VoiceBox preprocessed features}
\label{subsec:rnn}

Our other implementation was an RNN with a LSTM-50 layer, followed by a pooling layer, followed by a LSTM-40 and LSTM-30 and a dense layer with a validation accuracy of 68\%.
The features used were the features extracted from voicebox in the files \texttt{feat\_vec.mat}, which contained MFCCs for all speech signals.
For each frame of each signal, twelve MFCCs were calculated and stored in a large array.
The MFCCs were then resampled so they would be the same length across all speech signals.

The VoiceBox features performed poorly in a simple feed-forward neural network, but performed well in this network.
The performance of this is similar to the feed-forward neural network done previously.
Note that overfitting doesn’t seem to ever occur in this model, as the training accuracy seems to be bounded under 0.64.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 2.15cm]{figs/rnn_architecture}
	\caption{Architecture of the RNN implementation.}
	\label{fig:rnn_architecture}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width = 8.5cm]{figs/rnn_graph}
	\caption{A graph of averaged training accuracy versus testing accuracy over five different splits for the RNN implementation.}
	\label{fig:rnn_graph}
\end{figure}

\subsection{Test/training accuracies}
\label{subsec:tables}

The average training and test losses and accuracies for both the feed-forward neural network and RNN are given in \Cref{tab:avg_acc}.
\begin{table}[ht]
	\centering
	\begin{tabularx}{\linewidth}{|X|X|X|X|X|}
		\hline
		Ar\-chi\-tec\-ture & Training accuracy & Testing accuracy & Training loss & Testing loss \\
		\hline
		Feed-forward & 0.70 & 0.60 & 0.58 & 0.63 \\
		\hline
		RNN & 0.62 & 0.60 & 2.17 & 2.16 \\
		\hline
	\end{tabularx}
	\caption{Table of average training accuracies and losses for the feed-forward neural network and the RNN. Accuracies were recorded over randomly split testing and training data.}
	\label{tab:avg_acc}
\end{table}
From the table, we can see that testing accuracies for both types of networks were similar, though losses for the feed-forward neural network were much lower than that for the RNN.
The best training and test losses and accuracies we achieved are given in \Cref{tab:best_acc}.
\begin{table}[!ht]
	\centering
	\begin{tabularx}{\linewidth}{|X|X|X|X|X|}
		\hline
		Ar\-chi\-tec\-ture & Training accuracy & Testing accuracy & Training loss & Testing loss \\
		\hline
		Feed-forward & 0.69 & 0.69 & 0.59 & 0.61 \\
		\hline
		RNN & 0.61 & 0.72 & 2.42 & 2.42 \\
		\hline
	\end{tabularx}
	\caption{Table of best training accuracies and losses for the feed-forward neural network and the RNN observed over a single training/testing split. Accuracies over this run are given in \Cref{fig:best_ff}.}
	\label{tab:best_acc}
\end{table}
From the table, we see that the RNN performed slightly better than the feed-forward network during testing, despite having higher losses and a lower training accuracy.

\section{Libraries used}
\label{sec:libraries}

For this project, we tried out many different tools and libraries to extract features from our speech signals.

\texttt{VoiceBox}: We used VoiceBox to generate features such as sF0 - sF4, Energy, SHR on different frames of the speech signal, MFCCs, filter bank energies, and LPCs.
The features from VoiceBox performed poorly in a  feedforward neural network but performed well in the RNN architecture.

\texttt{VoiceSauce}: We experimented with the formants, pitch, and B1-B4 features from Praat as well as H1, H2, and H4, A1, A2, and A3, cepstral peak prominence, and harmonic-to-noise ratio.
However, none of these gave better performance than the VoiceBox features.

\texttt{fastdtw}: Calculation of overall Euclidean distance of one speech signal to another.
Created distance features that were experimented with, but unfortunately, did not perform well enough as stated in \Cref{subsec:normalization}.

\texttt{python\_speech\_features}: Used to create MFCC features (averages and standard deviations) that were used in our best-performing feed-forward neural network.

\texttt{DARLA}: Vowel extraction tool from Dartmouth.
Performs automatic and semi-automatic vowel extraction given a speech signal, a transcript of what was said in the signal, and some characteristics of the speaker.
Unfortunately, the tool performed too inconsistently for it to be useful in our project.

\section{Future work}
\label{sec:future}

Overall, the models could have been significantly improved with more data.
The project was restricted to use only the provided data set, which only had 170 signals, and also had inaccuracies in some classifications.
Some of the speakers labeled as native clearly struggled to speak the words and had accents not associated with any primarily English-speaking countries.
In other words, our data had “noisy-labelling”, where some signals were labeled as English speakers but had slow speech and with a non-English accent.
Additionally, since our dataset was rather small, more complex architectures would have been too difficult to train with such little data.
In the future, we would add significantly more data (thousands or tens of thousands of speakers) in order to test these models  and be more accurate in our labeling of native versus non-native English speakers.
This which would reduce the impact of noisy labelling and allow for the exploration of more complex architectures.

We would also have done more research on how the different architectures worked.
We tested many different configurations for the feed-forward neural network and RNN, but without a strong understanding of how they function we felt we were essentially making random guesses.
We experimented with the number of layers, the number of units in each layer, where and how often we should apply pooling, and other features of the network, but none of our changes improved the performance of the network.
Perhaps with a stronger background in neural networks we would have had a more intuitive understanding of the neural nets and we would have been able to experiment more thoughtfully and figure out how to shape the network to achieve what we wanted.

Lastly, we would have looked into more robust and augmented feature extraction.
The features we extracted were sensitive to the different lengths and sound qualities of the speech signals, so it is likely that our features themselves were very noisy.
In the future, we would look into methods to normalize the speech signals so that the features are as comparable as possible.
We explored this a bit with VoiceBox’s noise reduction and voice activity detection methods, but it did not perform as well as we needed it to and did not result in better performance.
We would look into more advanced noise reduction and voice activity detection as well as optimal ways to resample the speech signals so they are all the same length without significantly distorting the speech itself.
We would also have looked into more state-of-the-art methods of processing features for speaker recognition.
Recent research seems to look towards i-vectors and x-vectors as features extracted from the neural network and show promising results in speaker recognition \cite{ivector, xvector}.


\section{Conclusion}
\label{sec:conclusion}

Overall, it seems that the RNN had more consistent performance than our feed-forward neural network implementation.
This is likely because RNNs consider the relationship of features over time while feed-forward neural networks cannot learn these correlations as well.
However, the feed-forward neural network implementation significantly reduced complexity and was able to run more than 100 epochs in less than a minute, whereas the RNN architecture required an hour to run 100 epochs.
We’ve demonstrated two architectures that can achieve around 60\% testing accuracy on average on the dataset, given MFCCs as features.
When comparing the best results across multiple runs, the feed-forward network achieved up to 69\% testing accuracy and the RNN achieved up to 72\% testing accuracy.

\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
