{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoBdJNiyaMoC"
   },
   "source": [
    "Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "t6cfKHA7p3WP",
    "outputId": "90dcb3d6-6b1c-4834-85eb-5b9d08b5b61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vr1CQxtvaQTa"
   },
   "source": [
    "Change Directory to Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NX_Gzz72p-bH",
    "outputId": "0da83b7a-5701-41a3-aa46-0c542200835b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_vec.mat  feat_vec_rnn.mat\tlabels.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "#print(os.getcwd())\n",
    "os.chdir('/content/gdrive/My Drive/ECE114F19_SpeechMLproj')\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gT9QjbtWaWa-"
   },
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "OD-odBBaw613",
    "outputId": "9d449e36-ab02-4c19-954d-d368849116a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (144, 12)\n",
      "Test data shape: (26, 12)\n",
      "Training/Valid target shape: (144, 1)\n",
      "Test target shape: (26, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X=sio.loadmat('feat_vec.mat')\n",
    "y=sio.loadmat('labels')\n",
    "\n",
    "X_data=X['feat_vec']\n",
    "y_data=y['labels']\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test= train_test_split(X_data, y_data, test_size=0.15)\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.15)\n",
    "\n",
    "\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train_val.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_val.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UGP5EPvaYYJ"
   },
   "source": [
    "One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vQ6w6PHUzh_o",
    "outputId": "31198211-c366-4ab9-ab7e-d61cd0ad7b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "\n",
    "y_train = onehot_encoder.fit_transform(y_train)\n",
    "y_val = onehot_encoder.fit_transform(y_val)\n",
    "y_test = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRV3K8M5acwm"
   },
   "source": [
    "Run Fully Connected (FeedForward) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pKIRxQRg1PMn",
    "outputId": "836d17ad-22a3-444d-ae5c-ee1f0be230ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 122 samples, validate on 22 samples\n",
      "Epoch 1/100\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6908 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 0s 107us/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 0s 107us/step - loss: 0.6929 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 0s 98us/step - loss: 0.6880 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 0s 101us/step - loss: 0.6900 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 0s 126us/step - loss: 0.6906 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 0s 92us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 0s 164us/step - loss: 0.6886 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 0s 118us/step - loss: 0.6887 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 0s 130us/step - loss: 0.6872 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 0s 166us/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 0s 149us/step - loss: 0.6890 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 0s 178us/step - loss: 0.6885 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 0s 137us/step - loss: 0.6904 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 0s 120us/step - loss: 0.6896 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 0s 124us/step - loss: 0.6914 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 0s 101us/step - loss: 0.6891 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 0s 141us/step - loss: 0.6924 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 0s 109us/step - loss: 0.6892 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 0s 104us/step - loss: 0.6907 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 0s 129us/step - loss: 0.6902 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 0s 126us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 0s 134us/step - loss: 0.6926 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 0s 174us/step - loss: 0.6898 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 0s 160us/step - loss: 0.6905 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 0s 132us/step - loss: 0.6923 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 0s 143us/step - loss: 0.6917 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 0s 105us/step - loss: 0.6882 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 0s 140us/step - loss: 0.6913 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 0s 102us/step - loss: 0.6931 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 0s 111us/step - loss: 0.6910 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 0s 93us/step - loss: 0.6910 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 0s 113us/step - loss: 0.6886 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 0s 102us/step - loss: 0.6909 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 0s 94us/step - loss: 0.6894 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 0s 223us/step - loss: 0.6925 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 0s 124us/step - loss: 0.6905 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 0s 137us/step - loss: 0.6908 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 0s 208us/step - loss: 0.6910 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 0s 156us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 0s 108us/step - loss: 0.6906 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 0s 107us/step - loss: 0.6866 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 0s 131us/step - loss: 0.6898 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 0s 158us/step - loss: 0.6915 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 0s 104us/step - loss: 0.6931 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 0s 108us/step - loss: 0.6886 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 0s 102us/step - loss: 0.6920 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 0s 110us/step - loss: 0.6905 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 0s 102us/step - loss: 0.6894 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 0s 120us/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 0s 117us/step - loss: 0.6908 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 0s 112us/step - loss: 0.6909 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 0s 129us/step - loss: 0.6911 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 0s 159us/step - loss: 0.6909 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 0s 132us/step - loss: 0.6889 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 0s 172us/step - loss: 0.6919 - acc: 0.5328 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 0s 138us/step - loss: 0.6896 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 0s 115us/step - loss: 0.6914 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 0s 132us/step - loss: 0.6922 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 0s 119us/step - loss: 0.6905 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 0s 128us/step - loss: 0.6902 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 0s 137us/step - loss: 0.6888 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 0s 129us/step - loss: 0.6923 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 0s 92us/step - loss: 0.6902 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 0s 149us/step - loss: 0.6915 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 0s 163us/step - loss: 0.6916 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 0s 115us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 0s 124us/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 0s 138us/step - loss: 0.6865 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 0s 114us/step - loss: 0.6915 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 0s 148us/step - loss: 0.6919 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 0s 112us/step - loss: 0.6918 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 0s 176us/step - loss: 0.6889 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 0s 181us/step - loss: 0.6899 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 0s 126us/step - loss: 0.6919 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 0s 109us/step - loss: 0.6908 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 0s 95us/step - loss: 0.6901 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 0s 125us/step - loss: 0.6893 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 0s 111us/step - loss: 0.6906 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 0s 141us/step - loss: 0.6902 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 0s 146us/step - loss: 0.6886 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 0s 144us/step - loss: 0.6937 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 0s 100us/step - loss: 0.6912 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 0s 133us/step - loss: 0.6905 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 0s 154us/step - loss: 0.6879 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 0s 150us/step - loss: 0.6910 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 0s 140us/step - loss: 0.6874 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 0s 166us/step - loss: 0.6924 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 0s 120us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 0s 135us/step - loss: 0.6912 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 0s 123us/step - loss: 0.6906 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 0s 132us/step - loss: 0.6879 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 0s 108us/step - loss: 0.6906 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 0s 90us/step - loss: 0.6893 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 0s 90us/step - loss: 0.6919 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 0s 87us/step - loss: 0.6888 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 0s 90us/step - loss: 0.6900 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 0s 90us/step - loss: 0.6903 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 0s 96us/step - loss: 0.6911 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 0s 136us/step - loss: 0.6882 - acc: 0.5410 - val_loss: 0.6841 - val_acc: 0.5909\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='softmax', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))         \n",
    "\n",
    "Adam=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.9, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=100, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EebZ47qMakj3"
   },
   "source": [
    "Get validation and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "yuWhLcrO5WFB",
    "outputId": "2c5a638d-d257-45de-8a83-97a219b52a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 223us/step\n",
      "26/26 [==============================] - 0s 651us/step\n",
      "Validation [loss, accuracy] is  [0.6840943748300726, 0.5909091125835072]\n",
      "Test [loss, accuracy] is  [0.704076546889085, 0.4230769230769231]\n"
     ]
    }
   ],
   "source": [
    "score_val = model.evaluate(X_val, y_val, batch_size=20)\n",
    "score_test = model.evaluate(X_test, y_test, batch_size=20)\n",
    "\n",
    "print('Validation [loss, accuracy] is ', score_val)\n",
    "print('Test [loss, accuracy] is ', score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89iHOYkaapTo"
   },
   "source": [
    "Get data formatted for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "7b397hiNG0pY",
    "outputId": "9d76a759-5637-4fea-dc0a-9da4c7fa323a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 1363, 12)\n",
      "(170, 1)\n",
      "Training/Valid data shape: (144, 1363, 12)\n",
      "Test data shape: (26, 1363, 12)\n",
      "Training/Valid target shape: (144, 1)\n",
      "Test target shape: (26, 1)\n",
      "(122, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X=sio.loadmat('feat_vec_rnn.mat')\n",
    "y=sio.loadmat('labels')\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "X_data=X['feat_vec_rnn']\n",
    "X_data=np.transpose(X_data, (2, 0, 1))\n",
    "\n",
    "y_data=y['labels']\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test= train_test_split(X_data, y_data, test_size=0.15)\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.15)\n",
    "\n",
    "\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train_val.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_val.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(y_train)\n",
    "y_val = onehot_encoder.fit_transform(y_val)\n",
    "y_test = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXHr6znGasSG"
   },
   "source": [
    "Run LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "i3Nd9Djw7rZR",
    "outputId": "590b450e-cb6b-48d2-ca62-0e2dac752d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 122 samples, validate on 22 samples\n",
      "Epoch 1/15\n",
      "122/122 [==============================] - 12s 99ms/step - loss: 2.9184 - acc: 0.4836 - val_loss: 2.8819 - val_acc: 0.5000\n",
      "Epoch 2/15\n",
      "122/122 [==============================] - 5s 42ms/step - loss: 2.8798 - acc: 0.5410 - val_loss: 2.8618 - val_acc: 0.4091\n",
      "Epoch 3/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8677 - acc: 0.5738 - val_loss: 2.8498 - val_acc: 0.5909\n",
      "Epoch 4/15\n",
      "122/122 [==============================] - 5s 42ms/step - loss: 2.8539 - acc: 0.6230 - val_loss: 2.8413 - val_acc: 0.6364\n",
      "Epoch 5/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8519 - acc: 0.5738 - val_loss: 2.8350 - val_acc: 0.6364\n",
      "Epoch 6/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8454 - acc: 0.6230 - val_loss: 2.8299 - val_acc: 0.7273\n",
      "Epoch 7/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8392 - acc: 0.6148 - val_loss: 2.8256 - val_acc: 0.7273\n",
      "Epoch 8/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8407 - acc: 0.5820 - val_loss: 2.8219 - val_acc: 0.7273\n",
      "Epoch 9/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8358 - acc: 0.6148 - val_loss: 2.8190 - val_acc: 0.7273\n",
      "Epoch 10/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8331 - acc: 0.6066 - val_loss: 2.8166 - val_acc: 0.7273\n",
      "Epoch 11/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8238 - acc: 0.5984 - val_loss: 2.8144 - val_acc: 0.6818\n",
      "Epoch 12/15\n",
      "122/122 [==============================] - 5s 42ms/step - loss: 2.8263 - acc: 0.5984 - val_loss: 2.8122 - val_acc: 0.6364\n",
      "Epoch 13/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8157 - acc: 0.6393 - val_loss: 2.8101 - val_acc: 0.6364\n",
      "Epoch 14/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8245 - acc: 0.6066 - val_loss: 2.8082 - val_acc: 0.6364\n",
      "Epoch 15/15\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 2.8247 - acc: 0.6230 - val_loss: 2.8065 - val_acc: 0.6364\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "n_samples = X_train.shape[0]  # number of data points\n",
    "n_features = X_train.shape[1]  # dimension of feature vector for each sample\n",
    "time_steps = X_train.shape[2] # how many samples across time were taken\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='random_normal', input_shape=(n_features, time_steps)))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=(2), strides=2, padding='valid', data_format=None))\n",
    "\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(30, return_sequences=False))\n",
    "\n",
    "model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=0,l2=0.5)))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "Adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.99, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=100, epochs=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ajihnp6auW2"
   },
   "source": [
    "Get val and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "LIU-beqBPHhb",
    "outputId": "aab5739b-1c58-4457-bd29-583a5dc983b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 22ms/step\n",
      "26/26 [==============================] - 0s 18ms/step\n",
      "Validation [loss, accuracy] is  [2.806519378315319, 0.6363636146892201]\n",
      "Test [loss, accuracy] is  [2.8246509662041297, 0.692307710647583]\n"
     ]
    }
   ],
   "source": [
    "score_val = model.evaluate(X_val, y_val, batch_size=20)\n",
    "score_test = model.evaluate(X_test, y_test, batch_size=20)\n",
    "\n",
    "print('Validation [loss, accuracy] is ', score_val)\n",
    "print('Test [loss, accuracy] is ', score_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ECE114_Speech_ML_project_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
